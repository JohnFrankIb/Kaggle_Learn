{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Data\n",
    "full_data = pd.read_csv('train.csv',index_col='Id')\n",
    "full_test_data = pd.read_csv('test.csv',index_col='Id')\n",
    "\n",
    "# Obtain Target and Predictors\n",
    "features = ['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd']\n",
    "\n",
    "y = full_data.SalePrice\n",
    "\n",
    "x = full_data[features].copy()\n",
    "test_x = full_test_data[features].copy()\n",
    "\n",
    "# Break off Validation set from training data\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.8,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the Models\n",
    "model_1 = RandomForestRegressor(n_estimators=50,random_state=0)\n",
    "model_2 = RandomForestRegressor(n_estimators=100,random_state=0)\n",
    "model_3 = RandomForestRegressor(n_estimators=100,criterion='absolute_error',random_state=0)\n",
    "model_4 = RandomForestRegressor(n_estimators=200,min_samples_split=10,random_state=0)\n",
    "model_5 = RandomForestRegressor(n_estimators=100,max_depth=7,random_state=0)\n",
    "\n",
    "models = [model_1,model_2,model_3,model_4,model_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 MAE: 24015.492818003917\n",
      "Model 2 MAE: 23740.979228636657\n",
      "Model 3 MAE: 23528.78421232877\n",
      "Model 4 MAE: 23874.524641816766\n",
      "Model 5 MAE: 23706.672864217904\n"
     ]
    }
   ],
   "source": [
    "# Import for Validation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different models\n",
    "def score_model(model,x_tr=x_train,x_te=x_test,y_tr=y_train,y_te=y_test):\n",
    "    model.fit(x_tr,y_tr)\n",
    "    yhat = model.predict(x_te)\n",
    "    return mean_absolute_error(y_te,yhat)\n",
    "\n",
    "# Loop for Valid each Model\n",
    "for i in range(0,len(models)):\n",
    "    mae = score_model(models[i])\n",
    "    print(f'Model {i+1} MAE: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# Select Target\n",
    "y = data.Price\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "melb_predictors = data.drop(['Price'],axis=1)\n",
    "x = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Measure Quality of Each Approach\n",
    "def score_dataset(x_train,x_test,y_train,y_test):\n",
    "    model = RandomForestRegressor(n_estimators=10,random_state=0)\n",
    "    model.fit(x_train,y_train)\n",
    "    yhat = model.predict(x_test)\n",
    "    return mean_absolute_error(y_test,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Approach 1 | `Drop` Columns with `Missing Values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns with missing values are: ['Car', 'BuildingArea', 'YearBuilt']\n",
      "Approach 1: (Drop columns with Missing Values)\n",
      "183550.22137772635)\n"
     ]
    }
   ],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in x_train.columns if x_train[col].isnull().any()]\n",
    "print('The columns with missing values are:',cols_with_missing)\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_x_train = x_train.drop(cols_with_missing,axis=1)\n",
    "reduced_x_test = x_test.drop(cols_with_missing,axis=1)\n",
    "\n",
    "print(f'Approach 1: (Drop columns with Missing Values)\\n{score_dataset(reduced_x_train,reduced_x_test,y_train,y_test)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 2 | `Imputation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach 2: (Imputation)\n",
      "179816.89508731329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_x_train = pd.DataFrame(my_imputer.fit_transform(x_train))\n",
    "imputed_x_test = pd.DataFrame(my_imputer.fit_transform(x_test))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_x_train.columns = x_train.columns\n",
    "imputed_x_test.columns = x_test.columns\n",
    "\n",
    "print(f'Approach 2: (Imputation)\\n{score_dataset(imputed_x_train,imputed_x_test,y_train,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 3 | `Extension to Imputation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach 3: (Extension to Imputation)\n",
      "179986.2708570026\n"
     ]
    }
   ],
   "source": [
    "# Make copy to avoid changing original data\n",
    "x_train_plus = x_train.copy()\n",
    "x_test_plus = x_test.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    x_train_plus[col + '_was_missing'] = x_train_plus[col].isnull()\n",
    "    x_test_plus[col + '_was_missing'] = x_test_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_x_train_plus = pd.DataFrame(my_imputer.fit_transform(x_train_plus))\n",
    "imputed_x_test_plus = pd.DataFrame(my_imputer.fit_transform(x_test_plus))\n",
    "\n",
    "print(f'Approach 3: (Extension to Imputation)\\n{score_dataset(imputed_x_train_plus,imputed_x_test_plus,y_train,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Categorical Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# Get Target and Features\n",
    "y = data.Price\n",
    "x = data.drop(['Price'],axis=1)\n",
    "\n",
    "# Divide data into training and testing\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geting the name of columns with missing values to drop\n",
    "cols_with_missing = [col for col in x.columns if x[col].isnull().any()]\n",
    "\n",
    "# Droping in Training data\n",
    "x_train.drop(cols_with_missing,axis=1,inplace=True)\n",
    "\n",
    "# Droping in Testing data\n",
    "x_test.drop(cols_with_missing,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical columns with relatively low cardinality\n",
    "low_cardinality_cols = [cname for cname in x_train.columns if x_train[cname].nunique() < 10 and x_train[cname].dtype == 'object']\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in x_train.columns if x_train[cname].dtype in ['int64','float64']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "x_train = x_train[my_cols].copy()\n",
    "x_test = x_test[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12167</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3182.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-37.85984</td>\n",
       "      <td>144.9867</td>\n",
       "      <td>13240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>h</td>\n",
       "      <td>SA</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>-37.85800</td>\n",
       "      <td>144.9005</td>\n",
       "      <td>6380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>-37.79880</td>\n",
       "      <td>144.8220</td>\n",
       "      <td>3755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>u</td>\n",
       "      <td>SP</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3046.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>-37.70830</td>\n",
       "      <td>144.9158</td>\n",
       "      <td>8870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>-37.76230</td>\n",
       "      <td>144.8272</td>\n",
       "      <td>4217.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n",
       "12167    u      S  Southern Metropolitan      1       5.0    3182.0       1.0   \n",
       "6524     h     SA   Western Metropolitan      2       8.0    3016.0       2.0   \n",
       "8413     h      S   Western Metropolitan      3      12.6    3020.0       3.0   \n",
       "2919     u     SP  Northern Metropolitan      3      13.0    3046.0       3.0   \n",
       "6043     h      S   Western Metropolitan      3      13.3    3020.0       3.0   \n",
       "\n",
       "       Bathroom  Landsize  Lattitude  Longtitude  Propertycount  \n",
       "12167       1.0       0.0  -37.85984    144.9867        13240.0  \n",
       "6524        2.0     193.0  -37.85800    144.9005         6380.0  \n",
       "8413        1.0     555.0  -37.79880    144.8220         3755.0  \n",
       "2919        1.0     265.0  -37.70830    144.9158         8870.0  \n",
       "6043        1.0     673.0  -37.76230    144.8272         4217.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['Type', 'Method', 'Regionname']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variable\n",
    "s = (x_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print('Categorical variables:')\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to Measure Quality of Each Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for comparing different approaches\n",
    "def score_dataset(x_train,x_test,y_train,y_test):\n",
    "    model = RandomForestRegressor(n_estimators=100,random_state=0)\n",
    "    model.fit(x_train,y_train)\n",
    "    yhat = model.predict(x_test)\n",
    "    return mean_absolute_error(y_test,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Approach 1` (Drop Categorical Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop Categorical Variables):\n",
      "175703.48185157913\n"
     ]
    }
   ],
   "source": [
    "drop_x_train = x_train.select_dtypes(exclude='object')\n",
    "drop_x_test = x_test.select_dtypes(exclude='object')\n",
    "\n",
    "print('MAE from Approach 1 (Drop Categorical Variables):')\n",
    "print(score_dataset(drop_x_train,drop_x_test,y_train,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Aprroach 2` (Ordinal Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Ordinal Encoding):\n",
      "165936.40548390493\n"
     ]
    }
   ],
   "source": [
    "# Make copy to avoid changing original data\n",
    "label_x_train = x_train.copy()\n",
    "label_x_test = x_test.copy()\n",
    "\n",
    "# Apply Ordinal Encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_x_train[object_cols] = ordinal_encoder.fit_transform(x_train[object_cols])\n",
    "label_x_test[object_cols] = ordinal_encoder.transform(x_test[object_cols])\n",
    "\n",
    "# \n",
    "print('MAE from Approach 2 (Ordinal Encoding):')\n",
    "print(score_dataset(label_x_train,label_x_test,y_train,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Approach 3` (One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply <b>One-Hot Encoder</b> to each column with categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "166089.4893009678\n"
     ]
    }
   ],
   "source": [
    "# handle_unknown = avoid : to avoid errors\n",
    "# sparse=False ensures that the encoded columns are returnet as a numpy array\n",
    "\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore',sparse_output=False) \n",
    "\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(x_train[object_cols]))\n",
    "OH_cols_test = pd.DataFrame(OH_encoder.transform(x_test[object_cols]))\n",
    "\n",
    "# One-Hot Encoding removed index; put it back\n",
    "OH_cols_train.index = x_train.index\n",
    "OH_cols_test.index = x_test.index\n",
    "\n",
    "# Remove Categorical data columns (will replace with one-hot encoding)\n",
    "num_x_train = x_train.drop(object_cols,axis=1)\n",
    "num_x_test = x_test.drop(object_cols,axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_x_train = pd.concat([num_x_train,OH_cols_train],axis=1)\n",
    "OH_x_test = pd.concat([num_x_test,OH_cols_test],axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_x_train.columns = OH_x_train.columns.astype('str')\n",
    "OH_x_test.columns = OH_x_test.columns.astype('str')\n",
    "\n",
    "print('MAE from Approach 3 (One-Hot Encoding):')\n",
    "print(score_dataset(OH_x_train,OH_x_test,y_train,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
